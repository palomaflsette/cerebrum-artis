{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f154d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-paper')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9dffbd",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2c1706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_PATH = project_root / 'data' / 'artemis'\n",
    "CHECKPOINT_DIR = project_root / 'checkpoints'\n",
    "OUTPUT_DIR = project_root / 'outputs'\n",
    "\n",
    "# Create output directories\n",
    "(OUTPUT_DIR / 'figures').mkdir(parents=True, exist_ok=True)\n",
    "(OUTPUT_DIR / 'metrics').mkdir(parents=True, exist_ok=True)\n",
    "(OUTPUT_DIR / 'tables').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Emotion labels (ArtEmis)\n",
    "EMOTION_LABELS = [\n",
    "    'amusement', 'awe', 'contentment', 'excitement',\n",
    "    'anger', 'disgust', 'fear', 'sadness', 'something else'\n",
    "]\n",
    "\n",
    "# Model configurations\n",
    "MODELS = {\n",
    "    'V1': {\n",
    "        'name': 'V1 Baseline',\n",
    "        'checkpoint': CHECKPOINT_DIR / 'v1_baseline' / 'checkpoint_best.pt',\n",
    "        'expected_acc': 0.65\n",
    "    },\n",
    "    'V2': {\n",
    "        'name': 'V2 Improved',\n",
    "        'checkpoint': CHECKPOINT_DIR / 'v2_improved' / 'checkpoint_best.pt',\n",
    "        'expected_acc': 0.68\n",
    "    },\n",
    "    'V3': {\n",
    "        'name': 'V3 Fuzzy Features',\n",
    "        'checkpoint': CHECKPOINT_DIR / 'v3_fuzzy_features' / 'checkpoint_best.pt',\n",
    "        'expected_acc': 0.7063\n",
    "    },\n",
    "    'V4': {\n",
    "        'name': 'V4 Fuzzy Gating',\n",
    "        'checkpoint': CHECKPOINT_DIR / 'v4_fuzzy_gating' / 'checkpoint_best.pt',\n",
    "        'expected_acc': 0.7037\n",
    "    },\n",
    "    'V4.1': {\n",
    "        'name': 'V4.1 Integrated',\n",
    "        'checkpoint': CHECKPOINT_DIR / 'v4_1_integrated' / 'checkpoint_best.pt',\n",
    "        'expected_acc': 0.7040\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"✓ Configuration loaded\")\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Checkpoints: {CHECKPOINT_DIR}\")\n",
    "print(f\"Models to evaluate: {len(MODELS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345d62ff",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "\n",
    "Load the ArtEmis test set for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f78f1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement dataset loading\n",
    "# from cerebrum_artis.data import ArtEmisDataset\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# test_dataset = ArtEmisDataset(\n",
    "#     data_path=DATA_PATH,\n",
    "#     split='test',\n",
    "#     image_size=224\n",
    "# )\n",
    "\n",
    "# test_loader = DataLoader(\n",
    "#     test_dataset,\n",
    "#     batch_size=32,\n",
    "#     shuffle=False,\n",
    "#     num_workers=4\n",
    "# )\n",
    "\n",
    "# print(f\"✓ Test set loaded: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe1275a",
   "metadata": {},
   "source": [
    "## 3. Evaluate Each Model\n",
    "\n",
    "Run inference on all models and collect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e93f940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device='cuda'):\n",
    "    \"\"\"\n",
    "    Evaluate a single model on test set.\n",
    "    \n",
    "    Returns:\n",
    "        predictions: numpy array of predicted classes\n",
    "        probabilities: numpy array of class probabilities\n",
    "        ground_truth: numpy array of true labels\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc='Evaluating'):\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['emotion']\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    return (\n",
    "        np.array(all_preds),\n",
    "        np.array(all_probs),\n",
    "        np.array(all_labels)\n",
    "    )\n",
    "\n",
    "# TODO: Uncomment when models are ready\n",
    "# results = {}\n",
    "# \n",
    "# for model_key, config in MODELS.items():\n",
    "#     print(f\"\\nEvaluating {config['name']}...\")\n",
    "#     \n",
    "#     # Load model\n",
    "#     model = load_model(model_key, config['checkpoint'])\n",
    "#     \n",
    "#     # Evaluate\n",
    "#     preds, probs, labels = evaluate_model(model, test_loader)\n",
    "#     \n",
    "#     # Calculate metrics\n",
    "#     accuracy = (preds == labels).mean()\n",
    "#     \n",
    "#     results[model_key] = {\n",
    "#         'predictions': preds,\n",
    "#         'probabilities': probs,\n",
    "#         'labels': labels,\n",
    "#         'accuracy': accuracy\n",
    "#     }\n",
    "#     \n",
    "#     print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ecaa74",
   "metadata": {},
   "source": [
    "## 4. Calculate Metrics\n",
    "\n",
    "Compute comprehensive metrics for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace3dad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "def calculate_metrics(predictions, ground_truth, emotion_labels):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive metrics.\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(ground_truth, predictions)\n",
    "    \n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        ground_truth, predictions, average=None, labels=range(len(emotion_labels))\n",
    "    )\n",
    "    \n",
    "    cm = confusion_matrix(ground_truth, predictions)\n",
    "    \n",
    "    # Per-class metrics\n",
    "    per_class = pd.DataFrame({\n",
    "        'Emotion': emotion_labels,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'Support': support\n",
    "    })\n",
    "    \n",
    "    # Macro averages\n",
    "    macro_precision = precision.mean()\n",
    "    macro_recall = recall.mean()\n",
    "    macro_f1 = f1.mean()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'macro_precision': macro_precision,\n",
    "        'macro_recall': macro_recall,\n",
    "        'macro_f1': macro_f1,\n",
    "        'per_class': per_class,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "# TODO: Calculate metrics for all models\n",
    "# metrics = {}\n",
    "# for model_key, res in results.items():\n",
    "#     metrics[model_key] = calculate_metrics(\n",
    "#         res['predictions'],\n",
    "#         res['labels'],\n",
    "#         EMOTION_LABELS\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed75158",
   "metadata": {},
   "source": [
    "## 5. Results Summary\n",
    "\n",
    "Create a summary table comparing all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a122d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create summary table\n",
    "# summary = pd.DataFrame([\n",
    "#     {\n",
    "#         'Model': MODELS[k]['name'],\n",
    "#         'Accuracy': metrics[k]['accuracy'],\n",
    "#         'Macro Precision': metrics[k]['macro_precision'],\n",
    "#         'Macro Recall': metrics[k]['macro_recall'],\n",
    "#         'Macro F1': metrics[k]['macro_f1']\n",
    "#     }\n",
    "#     for k in MODELS.keys()\n",
    "# ])\n",
    "# \n",
    "# # Style table\n",
    "# summary_styled = summary.style.format({\n",
    "#     'Accuracy': '{:.2%}',\n",
    "#     'Macro Precision': '{:.2%}',\n",
    "#     'Macro Recall': '{:.2%}',\n",
    "#     'Macro F1': '{:.2%}'\n",
    "# }).background_gradient(subset=['Accuracy'], cmap='RdYlGn', vmin=0.6, vmax=0.75)\n",
    "# \n",
    "# display(summary_styled)\n",
    "# \n",
    "# # Save to CSV and LaTeX\n",
    "# summary.to_csv(OUTPUT_DIR / 'tables' / 'model_comparison.csv', index=False)\n",
    "# summary.to_latex(OUTPUT_DIR / 'tables' / 'model_comparison.tex', index=False, float_format='%.2f')\n",
    "\n",
    "print(\"Summary table will be displayed here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc2591e",
   "metadata": {},
   "source": [
    "## 6. Visualization: Model Comparison\n",
    "\n",
    "Bar chart comparing model accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0549100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create visualization\n",
    "# fig, ax = plt.subplots(figsize=(10, 6))\n",
    "# \n",
    "# models = [MODELS[k]['name'] for k in MODELS.keys()]\n",
    "# accuracies = [metrics[k]['accuracy'] for k in MODELS.keys()]\n",
    "# \n",
    "# bars = ax.bar(models, accuracies, color=sns.color_palette('husl', len(models)))\n",
    "# \n",
    "# # Add value labels on bars\n",
    "# for bar in bars:\n",
    "#     height = bar.get_height()\n",
    "#     ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "#             f'{height:.2%}',\n",
    "#             ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "# \n",
    "# ax.set_ylabel('Accuracy', fontsize=12)\n",
    "# ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "# ax.set_ylim(0.6, 0.75)\n",
    "# ax.grid(axis='y', alpha=0.3)\n",
    "# plt.xticks(rotation=45, ha='right')\n",
    "# plt.tight_layout()\n",
    "# \n",
    "# plt.savefig(OUTPUT_DIR / 'figures' / 'model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()\n",
    "\n",
    "print(\"Visualization will be displayed here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199caa28",
   "metadata": {},
   "source": [
    "## 7. Confusion Matrices\n",
    "\n",
    "Visualize confusion matrix for best model (V3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa32cc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot confusion matrix\n",
    "# def plot_confusion_matrix(cm, labels, title, save_path):\n",
    "#     fig, ax = plt.subplots(figsize=(10, 8))\n",
    "#     \n",
    "#     # Normalize\n",
    "#     cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#     \n",
    "#     sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues',\n",
    "#                 xticklabels=labels, yticklabels=labels,\n",
    "#                 square=True, cbar_kws={'label': 'Proportion'})\n",
    "#     \n",
    "#     plt.ylabel('True Emotion', fontsize=12)\n",
    "#     plt.xlabel('Predicted Emotion', fontsize=12)\n",
    "#     plt.title(title, fontsize=14, fontweight='bold')\n",
    "#     plt.tight_layout()\n",
    "#     \n",
    "#     plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "#     plt.show()\n",
    "# \n",
    "# # Plot for V3 (best model)\n",
    "# plot_confusion_matrix(\n",
    "#     metrics['V3']['confusion_matrix'],\n",
    "#     EMOTION_LABELS,\n",
    "#     'Confusion Matrix - V3 Fuzzy Features',\n",
    "#     OUTPUT_DIR / 'figures' / 'confusion_matrix_v3.png'\n",
    "# )\n",
    "\n",
    "print(\"Confusion matrix will be displayed here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ec2c4e",
   "metadata": {},
   "source": [
    "## 8. Save Results\n",
    "\n",
    "Save all metrics to JSON for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04ccf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# TODO: Save metrics\n",
    "# metrics_to_save = {}\n",
    "# for model_key, m in metrics.items():\n",
    "#     metrics_to_save[model_key] = {\n",
    "#         'accuracy': float(m['accuracy']),\n",
    "#         'macro_precision': float(m['macro_precision']),\n",
    "#         'macro_recall': float(m['macro_recall']),\n",
    "#         'macro_f1': float(m['macro_f1']),\n",
    "#         'per_class': m['per_class'].to_dict('records'),\n",
    "#         'confusion_matrix': m['confusion_matrix'].tolist()\n",
    "#     }\n",
    "# \n",
    "# with open(OUTPUT_DIR / 'metrics' / 'model_evaluation.json', 'w') as f:\n",
    "#     json.dump(metrics_to_save, f, indent=2)\n",
    "# \n",
    "# print(\"✓ Metrics saved to outputs/metrics/model_evaluation.json\")\n",
    "\n",
    "print(\"Metrics will be saved here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7b6524",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook evaluated all model versions and generated:\n",
    "- ✓ Accuracy metrics for each model\n",
    "- ✓ Confusion matrices\n",
    "- ✓ Per-class performance metrics\n",
    "- ✓ Comparison visualizations\n",
    "- ✓ LaTeX tables for paper\n",
    "\n",
    "**Next steps:**\n",
    "1. Run `02_ensemble_analysis.ipynb` for ensemble evaluation\n",
    "2. Run `03_fuzzy_features_analysis.ipynb` for feature analysis"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
