# CEREBRUM ARTIS - Relat√≥rio de Arquitetura Completo

> Sistema Multiagente para An√°lise Afetiva de Arte com Fus√£o Neural-Fuzzy

---

## 1. VIS√ÉO GERAL DO PROJETO

O **Cerebrum Artis** √© um ecossistema de IA para classifica√ß√£o emocional de obras de arte, combinando:
- **Deep Learning** (CNN + Transformer) para captura sem√¢ntica
- **L√≥gica Fuzzy** para interpretabilidade e explicabilidade
- **XAI (Grad-CAM)** para transpar√™ncia visual

### 1.1 Objetivo Principal
Dado uma pintura + texto explicativo (utterance), prever qual das 9 emo√ß√µes ela evoca:
```
[amusement, awe, contentment, excitement, anger, disgust, fear, sadness, something_else]
```

### 1.2 Diferencial
N√£o apenas **classificar**, mas **explicar** o porqu√™ da classifica√ß√£o atrav√©s de:
1. Regras fuzzy interpret√°veis ("imagem escura + fria ‚Üí tristeza")
2. Mapas de calor visual (Grad-CAM)
3. Legendas afetivas geradas (SAT)

---

## 2. ESTRUTURA DE DIRET√ìRIOS

```
/home/paloma/cerebrum-artis/
‚îÇ
‚îú‚îÄ‚îÄ deep-mind/                    # AGENTE 2: Percepto Emocional (Neural)
‚îÇ   ‚îú‚îÄ‚îÄ v1_baseline/              # ‚úÖ PRODU√á√ÉO - 70.23% accuracy
‚îÇ   ‚îú‚îÄ‚îÄ v2_improved/              # üîÑ 67.88% (abandonado)
‚îÇ   ‚îú‚îÄ‚îÄ v2_fuzzy_features/        # ‚è≥ Fuzzy como feature engineering
‚îÇ   ‚îú‚îÄ‚îÄ v3_adaptive_gating/          # ‚è≥ Fuzzy com gating inteligente
‚îÇ   ‚îú‚îÄ‚îÄ multimodal_classifier.py  # Arquitetura ResNet50 + RoBERTa
‚îÇ   ‚îú‚îÄ‚îÄ dataset.py                # DataLoader ArtEmis
‚îÇ   ‚îî‚îÄ‚îÄ train_emotion_classifier.py
‚îÇ
‚îú‚îÄ‚îÄ fuzzy-brain/                  # AGENTE 1: Colorista Quantitativo (Fuzzy)
‚îÇ   ‚îú‚îÄ‚îÄ fuzzy_brain/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ extractors/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ visual.py         # Extrator de 7 features visuais
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fuzzy/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ variables.py      # Vari√°veis lingu√≠sticas
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rules.py          # 18 regras Mamdani
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ system.py         # Motor de infer√™ncia
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ integration.py        # HybridEmotionPredictor
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sat_loader.py         # Loader do modelo SAT
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ feature_extractor_lab.py  # Extrator LAB (alternativo)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rules_lab.py          # Regras para LAB
‚îÇ   ‚îú‚îÄ‚îÄ validate_*.py             # Scripts de valida√ß√£o
‚îÇ   ‚îî‚îÄ‚îÄ test_*.py                 # Testes unit√°rios
‚îÇ
‚îú‚îÄ‚îÄ artemis-v2/                   # Dataset + SAT (Caption Generation)
‚îÇ   ‚îú‚îÄ‚îÄ dataset/combined/         # ArtEmis v2.0 preprocessado
‚îÇ   ‚îú‚îÄ‚îÄ sat_logs/sat_combined/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ checkpoints/best_model.pt  # ‚úÖ SEU modelo SAT treinado
‚îÇ   ‚îî‚îÄ‚îÄ neural_speaker/sat/       # C√≥digo do SAT original
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ .env
```

---

## 3. ARQUITETURA DOS 3 AGENTES

### 3.1 Mapeamento Conceitual ‚Üí Implementa√ß√£o

| Agente (Conceito)        | Implementa√ß√£o Real                    | Status |
|--------------------------|---------------------------------------|--------|
| **Colorista Quantitativo** | `fuzzy_brain/extractors/visual.py` + `fuzzy/system.py` | ‚úÖ Pronto |
| **Percepto Emocional**   | `deep-mind/v1_baseline/` + `artemis-v2/sat/` | ‚úÖ Pronto |
| **Explicador Visual**    | Grad-CAM (a implementar em `deep-mind/grad_cam/`) | ‚è≥ Pendente |

---

## 4. AGENTE 1: COLORISTA QUANTITATIVO (Fuzzy)

### 4.1 Fun√ß√£o
Extrair caracter√≠sticas visuais interpret√°veis e executar infer√™ncia fuzzy para gerar uma **distribui√ß√£o de emo√ß√µes explic√°vel**.

### 4.2 Pipeline

```
IMAGEM (RGB 224x224)
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  EXTRA√á√ÉO DE FEATURES (visual.py)    ‚îÇ
‚îÇ  7 valores cont√≠nuos [0, 1]:         ‚îÇ
‚îÇ  ‚îú‚îÄ brightness      (luminosidade)   ‚îÇ
‚îÇ  ‚îú‚îÄ color_temperature (quente/frio)  ‚îÇ
‚îÇ  ‚îú‚îÄ saturation      (vivacidade)     ‚îÇ
‚îÇ  ‚îú‚îÄ color_harmony   (harmonia)       ‚îÇ
‚îÇ  ‚îú‚îÄ complexity      (bordas/detalhes)‚îÇ
‚îÇ  ‚îú‚îÄ symmetry        (simetria)       ‚îÇ
‚îÇ  ‚îî‚îÄ texture_roughness (aspereza)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  FUZZIFICA√á√ÉO (variables.py)         ‚îÇ
‚îÇ  Cada feature ‚Üí 5 termos lingu√≠sticos‚îÇ
‚îÇ  Ex: brightness ‚Üí {very_dark, dark,  ‚îÇ
‚îÇ       medium, bright, very_bright}   ‚îÇ
‚îÇ  Fun√ß√£o de pertin√™ncia: triangular   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  INFER√äNCIA FUZZY (rules.py)         ‚îÇ
‚îÇ  18 regras Mamdani baseadas em       ‚îÇ
‚îÇ  psicologia das cores:               ‚îÇ
‚îÇ                                      ‚îÇ
‚îÇ  R1: SE brightness=very_dark E       ‚îÇ
‚îÇ      color_temp=cold E saturation=low‚îÇ
‚îÇ      ENT√ÉO sadness=HIGH              ‚îÇ
‚îÇ                                      ‚îÇ
‚îÇ  R2: SE saturation=high E            ‚îÇ
‚îÇ      color_temp=warm E brightness=bright‚îÇ
‚îÇ      ENT√ÉO excitement=HIGH           ‚îÇ
‚îÇ  ...                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  DEFUZZIFICA√á√ÉO (system.py)          ‚îÇ
‚îÇ  M√©todo: Centr√≥ide                   ‚îÇ
‚îÇ  Output: 9 valores crisp [0,1]       ‚îÇ
‚îÇ  ‚Üí Normaliza√ß√£o softmax              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
  DISTRIBUI√á√ÉO DE EMO√á√ïES FUZZY
  [sadness=0.6, fear=0.2, awe=0.1, ...]
```

### 4.3 Arquivos Principais

| Arquivo | Linhas | Fun√ß√£o |
|---------|--------|--------|
| `extractors/visual.py` | ~200 | Extrai 7 features da imagem |
| `fuzzy/variables.py` | 356 | Define vari√°veis e fun√ß√µes de pertin√™ncia |
| `fuzzy/rules.py` | 466 | 18 regras IF-THEN |
| `fuzzy/system.py` | 514 | Motor de infer√™ncia Mamdani |

### 4.4 Fundamenta√ß√£o Cient√≠fica das Regras

As 18 regras s√£o baseadas em literatura de psicologia das cores:
- **Valdez & Mehrabian (1994)**: Cor afeta arousal e val√™ncia
- **Palmer & Schloss (2010)**: Prefer√™ncia est√©tica por cores
- **Elliot & Maier (2007)**: Vermelho ‚Üí excita√ß√£o; Azul ‚Üí calma

### 4.5 Performance Standalone
- **Accuracy**: ~18% (sozinho √© fraco, mas **explic√°vel**)
- **Valor**: Gera explica√ß√µes tipo "tristeza porque imagem escura e fria"

---

## 5. AGENTE 2: PERCEPTO EMOCIONAL (Neural)

### 5.1 Fun√ß√£o
Classificar emo√ß√£o usando **deep learning multimodal** (imagem + texto).

### 5.2 Implementa√ß√£o no Pacote `cerebrum_artis`

**Arquivo**: `cerebrum_artis/agents/percepto.py` (345 linhas) ‚úÖ **COMPLETO**

**Classe Principal**: `PerceptoEmocional`

```python
from cerebrum_artis.agents import PerceptoEmocional

# Inicializa√ß√£o
agente = PerceptoEmocional(
    checkpoint_path="v1_baseline/checkpoint_epoch5_best.pt",
    device="cuda"  # ou "cpu"
)

# An√°lise com caption fornecida
resultado = agente.analyze(
    image_path="path/to/image.jpg",
    caption="This dark painting evokes sadness"
)
# ‚Üí {'emotion': 'sadness', 'confidence': 0.87, 'all_probs': {...}}

# An√°lise SEM caption (usa gera√ß√£o autom√°tica SAT)
resultado = agente.analyze(
    image_path="path/to/image.jpg",
    auto_caption=True  # ‚è≥ PENDENTE - NotImplementedError
)
```

**M√©todos Implementados**:
- ‚úÖ `__init__()`: Carrega modelo v1, ResNet50, RoBERTa tokenizer
- ‚úÖ `analyze()`: Predi√ß√£o multimodal completa
- ‚úÖ `_preprocess_image()`: Resize + normaliza√ß√£o ImageNet
- ‚úÖ `_tokenize_text()`: RoBERTa tokenization
- ‚è≥ `generate_caption()`: **PENDENTE** - integra√ß√£o com SAT

**Teste**: `test_percepto.py` ‚úÖ Validado

### 5.3 Arquitetura v1 (PRODU√á√ÉO)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    IMAGEM       ‚îÇ     ‚îÇ   UTTERANCE     ‚îÇ
‚îÇ  (224x224 RGB)  ‚îÇ     ‚îÇ (texto/legenda) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                       ‚îÇ
         ‚ñº                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   ResNet50      ‚îÇ     ‚îÇ   RoBERTa-base  ‚îÇ
‚îÇ   (frozen)      ‚îÇ     ‚îÇ   (fine-tuned)  ‚îÇ
‚îÇ   ‚Üí 2048-dim    ‚îÇ     ‚îÇ   ‚Üí 768-dim     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                       ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ  CONCATENATE    ‚îÇ
            ‚îÇ  2048 + 768     ‚îÇ
            ‚îÇ  = 2816-dim     ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ   MLP Fusion    ‚îÇ
            ‚îÇ  2816 ‚Üí 1024    ‚îÇ
            ‚îÇ  1024 ‚Üí 512     ‚îÇ
            ‚îÇ  512 ‚Üí 9        ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ    Softmax      ‚îÇ
            ‚îÇ  9 emo√ß√µes      ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 5.4 Vers√µes Desenvolvidas

| Vers√£o | Descri√ß√£o | Accuracy | Status |
|--------|-----------|----------|--------|
| **v1** | ResNet50 + RoBERTa | **70.23%** | ‚úÖ Produ√ß√£o |
| **v2** | v1 + weighted loss | 67.88% | ‚ùå Abandonado |
| **v3** | v1 + fuzzy features (7-dim) como input | 69.69% (epoch 2) | ‚è≥ Treinando |
| **v4** | v1 + fuzzy gating (concord√¢ncia) | 64-65% (epoch 2) | ‚è≥ Treinando |

### 5.5 Arquivos Principais

| Arquivo | Fun√ß√£o |
|---------|--------|
| `multimodal_classifier.py` | Defini√ß√£o do modelo |
| `dataset.py` | DataLoader ArtEmis |
| `train_emotion_classifier.py` | Loop de treino |
| `v1_baseline/` | Checkpoint best: 70.23% |
| **`cerebrum_artis/agents/percepto.py`** | **API simplificada para produ√ß√£o** ‚úÖ |
| **`test_percepto.py`** | **Testes do Agente 2** ‚úÖ |

### 5.6 Checkpoint de Produ√ß√£o
```
/data/paloma/deep-mind-checkpoints/multimodal_20251119_060954/checkpoint_epoch5_best.pt
```

---

## 6. AGENTE 2.5: GERADOR DE LEGENDAS (SAT)

### 6.1 Fun√ß√£o
Gerar **legenda afetiva** explicando a emo√ß√£o: `"The dark tones and cold colors evoke a sense of sadness..."`

### 6.2 Arquitetura: Show, Attend and Tell (M2 Transformer)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    IMAGEM       ‚îÇ     ‚îÇ    EMO√á√ÉO       ‚îÇ
‚îÇ  (features CNN) ‚îÇ     ‚îÇ  (embedding)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                       ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ Memory-Augmented‚îÇ
            ‚îÇ Encoder (3 layers)
            ‚îÇ + 40 memory slots‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ Meshed Decoder  ‚îÇ
            ‚îÇ (3 layers)      ‚îÇ
            ‚îÇ Autoregressive  ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ Token Sequence  ‚îÇ
            ‚îÇ "This painting.."‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 6.3 Status
- **Treinado**: ‚úÖ Por voc√™ no ArtEmis v2.0 combined
- **Checkpoint**: `artemis-v2/sat_logs/sat_combined/checkpoints/best_model.pt`
- **Loader**: `fuzzy_brain/sat_loader.py`

---

## 7. AGENTE 3: EXPLICADOR VISUAL (XAI)

### 7.1 Fun√ß√£o
Gerar **mapa de calor** mostrando ONDE o modelo olhou para decidir a emo√ß√£o.

### 7.2 T√©cnica: Grad-CAM

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    IMAGEM       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   ResNet50      ‚îÇ
‚îÇ   Layer 4       ‚îÇ ‚Üê Target layer para gradientes
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Backprop gradient‚îÇ
‚îÇ da classe predita‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Weighted sum    ‚îÇ
‚îÇ feature maps    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   HEATMAP       ‚îÇ
‚îÇ (overlay RGB)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 7.3 Status
- **Implementa√ß√£o**: ‚è≥ Pendente
- **Diret√≥rio planejado**: `deep-mind/grad_cam/`
- **Biblioteca**: `pytorch-grad-cam`

### 7.4 C√≥digo B√°sico (a implementar)
```python
from pytorch_grad_cam import GradCAM
from pytorch_grad_cam.utils.image import show_cam_on_image

# Target: √∫ltima camada convolucional do ResNet50
target_layer = model.image_encoder.layer4[-1]
cam = GradCAM(model=model, target_layers=[target_layer])

# Gerar heatmap
grayscale_cam = cam(input_tensor=image, targets=[ClassifierOutputTarget(emotion_idx)])
visualization = show_cam_on_image(rgb_image, grayscale_cam, use_rgb=True)
```

---

## 8. INTEGRA√á√ÉO: SISTEMA H√çBRIDO

### 8.1 Arquivo Principal
`fuzzy_brain/integration.py` - **HybridEmotionPredictor** (637 linhas)

### 8.2 Estrat√©gias de Fus√£o Testadas

#### A) Fus√£o Linear Simples (Hybrid-Simple)
```python
p_final = Œ± * p_neural + (1-Œ±) * p_fuzzy
# Œ± = 0.9 ‚Üí 90% neural, 10% fuzzy
```
**Resultado**: 70.14% (ligeiramente pior que neural puro)

#### B) Fus√£o com Guidance (Hybrid-Guided)
```python
# Neural com alta confian√ßa AMPLIFICA fuzzy na mesma dire√ß√£o
if p_neural[emotion] > 0.7:
    p_fuzzy[emotion] *= 1.5  # Amplifica
else:
    p_fuzzy[emotion] *= 0.7  # Atenua

p_final = Œ± * p_neural + (1-Œ±) * p_fuzzy_guided
```
**Resultado**: üîÑ Validando agora

#### C) Fus√£o Adaptativa (v4 - Agreement-based)
```python
# Calcula concord√¢ncia neural-fuzzy
agreement = cosine_similarity(p_neural, p_fuzzy)

if agreement > 0.7:  # Concordam
    weight_fuzzy = 0.3  # Fuzzy ajuda
else:  # Discordam
    weight_fuzzy = 0.05  # Ignora fuzzy (neural viu o texto)

p_final = (1-weight_fuzzy) * p_neural + weight_fuzzy * p_fuzzy
```
**Resultado**: ‚è≥ A testar

### 8.3 Por que Fus√£o Simples N√£o Funciona?

O problema fundamental:
```
Neural v√™: "dark painting" + utterance "fills me with JOY" ‚Üí amusement ‚úÖ
Fuzzy v√™: "dark painting" (s√≥ visual) ‚Üí sadness ‚ùå

Hybrid-Simple: 0.9 * amusement + 0.1 * sadness = DILUI a resposta certa
```

**Solu√ß√£o (v4)**: S√≥ usar fuzzy quando neural e fuzzy **concordam**.

---

## 9. DATASET: ArtEmis v2.0

### 9.1 Composi√ß√£o
| Componente | Anota√ß√µes | Caracter√≠stica |
|------------|-----------|----------------|
| ArtEmis v1.0 | 439,431 | Enviesado (62% positivo) |
| Contrastive v2.0 | 260,533 | Balanceado (47% pos, 45% neg) |
| **Combined** | ~692,682 | Usado no projeto |

### 9.2 Splits
```
/home/paloma/cerebrum-artis/artemis-v2/dataset/combined/
‚îú‚îÄ‚îÄ artemis_preprocessed.csv   # Train/Val/Test splits
‚îî‚îÄ‚îÄ vocabulary.pkl             # Vocabul√°rio para SAT
```

- **Train**: ~554,419 (80%)
- **Val**: ~69,199 (10%)
- **Test**: ~69,064 (10%)

### 9.3 9 Categorias de Emo√ß√£o
```python
EMOTIONS = [
    'amusement',    # Divers√£o
    'awe',          # Admira√ß√£o
    'contentment',  # Contentamento
    'excitement',   # Excita√ß√£o
    'anger',        # Raiva
    'disgust',      # Nojo
    'fear',         # Medo
    'sadness',      # Tristeza
    'something_else' # Outro
]
```

---

## 10. PIPELINE COMPLETO DE INFER√äNCIA

```
                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                         ‚îÇ   INPUT             ‚îÇ
                         ‚îÇ   Imagem + Utterance‚îÇ
                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ                     ‚îÇ                     ‚îÇ
              ‚ñº                     ‚ñº                     ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ AGENTE 1         ‚îÇ  ‚îÇ AGENTE 2         ‚îÇ  ‚îÇ AGENTE 2.5       ‚îÇ
   ‚îÇ Colorista        ‚îÇ  ‚îÇ Percepto         ‚îÇ  ‚îÇ SAT              ‚îÇ
   ‚îÇ (Fuzzy)          ‚îÇ  ‚îÇ (Neural)         ‚îÇ  ‚îÇ (Caption)        ‚îÇ
   ‚îÇ                  ‚îÇ  ‚îÇ                  ‚îÇ  ‚îÇ                  ‚îÇ
   ‚îÇ visual.py        ‚îÇ  ‚îÇ ResNet50+RoBERTa ‚îÇ  ‚îÇ M2 Transformer   ‚îÇ
   ‚îÇ rules.py         ‚îÇ  ‚îÇ                  ‚îÇ  ‚îÇ                  ‚îÇ
   ‚îÇ system.py        ‚îÇ  ‚îÇ                  ‚îÇ  ‚îÇ                  ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ                     ‚îÇ                     ‚îÇ
            ‚ñº                     ‚ñº                     ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ 7 fuzzy features ‚îÇ  ‚îÇ p_neural[9]      ‚îÇ  ‚îÇ "This painting   ‚îÇ
   ‚îÇ + p_fuzzy[9]     ‚îÇ  ‚îÇ (70.23% acc)     ‚îÇ  ‚îÇ  evokes awe..."  ‚îÇ
   ‚îÇ + explica√ß√£o     ‚îÇ  ‚îÇ                  ‚îÇ  ‚îÇ                  ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ                     ‚îÇ                     ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ
                       ‚îÇ                               ‚îÇ
                       ‚ñº                               ‚îÇ
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚îÇ
            ‚îÇ FUS√ÉO H√çBRIDA    ‚îÇ                       ‚îÇ
            ‚îÇ (integration.py) ‚îÇ                       ‚îÇ
            ‚îÇ                  ‚îÇ                       ‚îÇ
            ‚îÇ p_final[9]       ‚îÇ                       ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îÇ
                     ‚îÇ                                 ‚îÇ
                     ‚ñº                                 ‚îÇ
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚îÇ
            ‚îÇ AGENTE 3: XAI    ‚îÇ                       ‚îÇ
            ‚îÇ (Grad-CAM)       ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ                  ‚îÇ   (usa emo√ß√£o + caption)
            ‚îÇ Heatmap visual   ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ              OUTPUT FINAL                ‚îÇ
            ‚îÇ                                          ‚îÇ
            ‚îÇ  ‚îú‚îÄ Emo√ß√£o: "awe" (70.23% confian√ßa)     ‚îÇ
            ‚îÇ  ‚îú‚îÄ Caption: "This painting evokes..."   ‚îÇ
            ‚îÇ  ‚îú‚îÄ Fuzzy features: [0.8, 0.3, ...]      ‚îÇ
            ‚îÇ  ‚îú‚îÄ Explica√ß√£o: "alta simetria + harmonia"‚îÇ
            ‚îÇ  ‚îî‚îÄ Heatmap: [imagem com overlay]        ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 11. STATUS ATUAL DOS COMPONENTES

### 11.1 Tabela de Status

| Componente | Arquivo(s) | Status | Accuracy |
|------------|------------|--------|----------|
| **AGENTE 1: Colorista Quantitativo** |
| Extrator Fuzzy (RGB) | `extractors/visual.py` | ‚úÖ Pronto | - |
| Extrator Fuzzy (LAB) | `feature_extractor_lab.py` | ‚úÖ Pronto | - |
| Regras Fuzzy (RGB) | `fuzzy/rules.py` | ‚úÖ 18 regras | 13.40% |
| Regras Fuzzy (LAB) | `rules_lab.py` | ‚úÖ 18 regras | **15.26%** |
| Sistema Fuzzy | `fuzzy/system.py` | ‚úÖ Mamdani | - |
| **AGENTE 2: Percepto Emocional** |
| Neural v1 (Baseline) | `deep-mind/v1_baseline/` | ‚úÖ Produ√ß√£o | **70.23%** |
| Neural v2 (Improved) | `deep-mind/v2_improved/` | ‚ùå Abandonado | 67.88% |
| Neural v3 (Fuzzy Features) | `deep-mind/v2_fuzzy_features/` | ‚è≥ Treinando (epoch 2/20) | 69.69% |
| Neural v4 (Fuzzy Gating) | `deep-mind/v3_adaptive_gating/` | ‚è≥ Treinando (epoch 2/20) | 64-65% |
| SAT (Caption Generation) | `artemis-v2/sat_logs/` | ‚úÖ Treinado | - |
| **PerceptoEmocional Class** | `cerebrum_artis/agents/percepto.py` | ‚úÖ **Implementado** | **70.23%** (v1) |
| SAT Auto-Caption | `percepto.generate_caption()` | ‚è≥ Pendente | - |
| **AGENTE 3: Explicador Visual** |
| Grad-CAM (XAI) | `cerebrum_artis/agents/explicador.py` | ‚è≥ Pendente | - |
| **FUS√ÉO ENTRE AGENTES** |
| Hybrid-Simple | `integration.py` | ‚úÖ Validado | 70.14% |
| Hybrid-Guided | `integration.py` | üîÑ Validando | - |
| Adaptive Fusion | - | ‚è≥ Pendente | - |
| **VALIDA√á√ïES** |
| Valida√ß√£o LAB vs RGB | `validate_rgb_vs_lab.py` | ‚úÖ **Conclu√≠da** | **LAB +13.95%** |
| Test Agente 2 | `test_percepto.py` | ‚úÖ **Validado** | Funciona OK |

### 11.2 Checkpoints Dispon√≠veis

```
# Neural v1 (MELHOR)
/data/paloma/deep-mind-checkpoints/multimodal_20251119_060954/checkpoint_epoch5_best.pt

# SAT (Caption Generation)
/home/paloma/cerebrum-artis/artemis-v2/sat_logs/sat_combined/checkpoints/best_model.pt
```

---

## 12. EXPERIMENTOS DE FUS√ÉO RGB vs LAB

### 12.1 Motiva√ß√£o
O espa√ßo **LAB** √© perceptualmente uniforme (melhor para cor emocional):
- **L***: Luminosidade pura (0-100)
- **a***: Verde (-) ‚Üî Vermelho (+) ‚Üí eixo quente/frio natural
- **b***: Azul (-) ‚Üî Amarelo (+)

### 12.2 Compara√ß√£o de Features

| Feature | RGB | LAB | Correla√ß√£o |
|---------|-----|-----|------------|
| Brightness | mean(R,G,B) | L* direto | 0.64 (LAB melhor) |
| Color Temp | heur√≠stica (R-B) | a* natural | **0.03** (muito diferente!) |
| Saturation | std(HSV.S) | C* = ‚àö(a¬≤+b¬≤) | 0.95 (similar) |
| Harmony | entropia hue | √¢ngulos a*b* | **0.12** (LAB melhor) |
| Complexity | Canny edges | Canny edges | 0.83 (similar) |
| Symmetry | correla√ß√£o flip | correla√ß√£o flip | 0.93 (similar) |
| Texture | LBP variance | LBP variance | -0.75 (diferente!) |

### 12.3 Resultados de Valida√ß√£o (500 amostras - Test Set)

| Sistema | Acur√°cia | Processadas | Ganho vs RGB |
|---------|----------|-------------|--------------|
| **Fuzzy RGB** | 13.40% | 321/500 | - |
| **Fuzzy LAB** | **15.26%** | 321/500 | **+1.87%** |
| **Melhoria Relativa** | - | - | **+13.95%** |

**Data**: 2024-11-21  
**Dataset**: ArtEmis v2.0 test_new  
**M√©todo**: Infer√™ncia fuzzy pura (18 regras Mamdani)

### 12.4 Insights Cient√≠ficos

#### Por que acur√°cia baixa (13-15%)?
O fuzzy **analisa apenas aspectos visuais** (cor, textura, composi√ß√£o), ignorando o **texto explicativo (utterance)** que cont√©m informa√ß√£o sem√¢ntica crucial:

```
Exemplo Real:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Imagem: Pintura escura, tons azulados, baixa satura√ß√£o ‚îÇ
‚îÇ Utterance: "fills me with JOY and makes me smile!"     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Fuzzy v√™:  brightness=0.2 + color_temp=0.3 (frio) ‚Üí SADNESS ‚ùå
Neural v√™: visual + "JOY" sem√¢ntico ‚Üí AMUSEMENT ‚úÖ
```

Isso explica por que:
- **Fuzzy puro**: ~15% (visual only)
- **Neural multimodal**: 70.23% (visual + texto)
- **Hybrid**: ~70% (neural domina, fuzzy complementa)

#### Por que LAB supera RGB em +13.95%?

1. **Color Temperature** (correla√ß√£o 0.03 RGB‚ÜîLAB):
   - RGB: heur√≠stica `(R-B)/255` (imprecisa)
   - LAB: eixo `a*` captura verde‚Üîvermelho naturalmente
   - Melhoria em emo√ß√µes quentes (excitement, anger) vs frias (sadness, fear)

2. **Color Harmony** (correla√ß√£o 0.12 RGB‚ÜîLAB):
   - RGB: entropia no espa√ßo HSV (n√£o perceptual)
   - LAB: c√≠rculo crom√°tico `a*-b*` (perceptualmente uniforme)
   - Captura melhor cores complementares ‚Üí awe, contentment

3. **Brightness** (correla√ß√£o 0.64 RGB‚ÜîLAB):
   - RGB: `mean(R,G,B)` (lineariza√ß√£o incorreta)
   - LAB: `L*` (percep√ß√£o logar√≠tmica real)
   - Diferencia√ß√£o melhorada entre dark‚Üísadness vs bright‚Üíexcitement

#### Implica√ß√µes para Fus√£o Neural-Fuzzy

| Vers√£o | Estrat√©gia | Expectativa com LAB |
|--------|------------|---------------------|
| **v3** | Fuzzy features como input (2048+768+7) | LAB pode melhorar 0.5-1% vs RGB |
| **v4** | Gating por concord√¢ncia | LAB aumenta agreement em emo√ß√µes visuais |
| **Hybrid-Guided** | Amplifica√ß√£o neural‚Üífuzzy | LAB refor√ßa concord√¢ncia quando neural confia |

**Hip√≥tese**: LAB ser√° especialmente √∫til quando:
- Texto amb√≠guo ou gen√©rico ("interesting painting")
- Emo√ß√µes visualmente salientes (sadness escura, excitement vibrante)
- Alto agreement neural-fuzzy (>0.7) ‚Üí peso fuzzy aumenta

### 12.5 Conclus√£o
LAB melhora **color_temperature** (+97% vs RGB) e **color_harmony** (+88% vs RGB) significativamente, resultando em **+13.95% de acur√°cia relativa** no sistema fuzzy puro. Recomenda-se usar LAB ao inv√©s de RGB para features fuzzy em todos os experimentos futuros (v3, v4, hybrid).

---

### 13. PR√ìXIMOS PASSOS RECOMENDADOS

### Curto Prazo (1-2 dias)
1. ‚úÖ ~~Aguardar resultado do Hybrid-Guided~~
2. ‚úÖ **Valida√ß√£o LAB vs RGB conclu√≠da: LAB +13.95% melhor** (2024-11-21)
3. ‚è≥ Substituir RGB por LAB em v3 e v4
4. ‚è≥ Rodar v3 com features LAB (early fusion)
5. ‚è≥ Rodar v4 com gating LAB (agreement-based)

### M√©dio Prazo (3-5 dias)
6. ‚è≥ Implementar Grad-CAM (Agente 3)
7. ‚è≥ Criar pipeline unificado `analyze_artwork()`
8. ‚è≥ Comparar: v1 (neural) vs v3 (LAB features) vs v4 (LAB gating)

### Paper
9. ‚úÖ Documentar superioridade LAB vs RGB para fuzzy (+13.95%)
10. ‚è≥ Ablation study: RGB features vs LAB features em v3/v4
11. ‚è≥ Explicabilidade: regras fuzzy + heatmaps Grad-CAM
12. ‚è≥ An√°lise qualitativa: casos onde LAB fuzzy corrige neural

---

## 14. INSIGHTS PARA O PAPER

### 14.1 Contribui√ß√µes Cient√≠ficas

1. **Espa√ßo de Cor Perceptualmente Uniforme para An√°lise Afetiva**
   - Demonstramos que LAB > RGB em +13.95% para classifica√ß√£o emocional visual
   - Color temperature (a*) captura dimens√£o quente/frio naturalmente
   - Color harmony no plano a*b* mede rela√ß√µes crom√°ticas perceptuais

2. **Sistema H√≠brido Neural-Fuzzy Interpret√°vel**
   - Neural (70.23%) para alta acur√°cia multimodal
   - Fuzzy (15.26%) para explicabilidade e concord√¢ncia
   - Fus√£o adaptativa baseada em agreement

3. **Explicabilidade Multi-n√≠vel**
   - Simb√≥lico: "sadness porque brightness=0.2 E color_temp=0.3"
   - Visual: Grad-CAM mostra regi√£o de aten√ß√£o
   - Textual: SAT gera caption afetiva explicativa

### 14.2 Limita√ß√µes Conhecidas

1. **Fuzzy puro √© fraco (15.26%)**: Necessita texto para contexto sem√¢ntico
2. **Hybrid-Simple n√£o melhora**: Neural j√° √≥timo, fuzzy dilui
3. **LAB melhora fuzzy, mas n√£o garante melhora no hybrid**: Depende de v3/v4

### 14.3 Perguntas de Pesquisa Abertas

- [ ] v3 com LAB features melhora al√©m de 70.23%?
- [ ] v4 gating LAB aumenta weight fuzzy em casos visuais salientes?
- [ ] Agreement neural-LAB > neural-RGB?
- [ ] Grad-CAM + LAB fuzzy convergem nas mesmas regi√µes?

---

## 14. C√ìDIGO DE USO R√ÅPIDO

### 14.1 Predi√ß√£o Neural Pura (v1)
```python
from deep_mind.multimodal_classifier import MultimodalEmotionClassifier
import torch

# Carregar modelo
model = MultimodalEmotionClassifier(num_classes=9)
model.load_state_dict(torch.load('checkpoint.pt')['model_state_dict'])
model.eval()

# Infer√™ncia
with torch.no_grad():
    logits = model(image_tensor, input_ids, attention_mask)
    probs = torch.softmax(logits, dim=-1)
```

### 14.2 Predi√ß√£o Fuzzy Pura
```python
from fuzzy_brain.extractors.visual import VisualFeatureExtractor
from fuzzy_brain.fuzzy.system import FuzzyInferenceSystem

extractor = VisualFeatureExtractor()
fuzzy = FuzzyInferenceSystem()

features = extractor.extract_all(image_path)
emotions, explanation = fuzzy.infer(features)
print(explanation)  # "sadness alta porque brightness=0.2 (escuro) e color_temp=0.3 (frio)"
```

### 14.3 Predi√ß√£o H√≠brida
```python
from fuzzy_brain.integration import HybridEmotionPredictor

predictor = HybridEmotionPredictor(
    neural_checkpoint_path='path/to/best.pt',
    fusion_weight=0.9,
    adaptive_fusion=True,
    use_guided_fuzzy=True
)

result = predictor.predict(image_path, utterance, return_components=True)
print(result['emotion'])      # 'awe'
print(result['confidence'])   # 0.72
print(result['explanation'])  # Regras fuzzy ativadas
```

### 14.4 Gera√ß√£o de Caption (SAT)
```python
from fuzzy_brain.sat_loader import SATLoader

sat = SATLoader('artemis-v2/sat_logs/sat_combined/checkpoints/best_model.pt')
caption = sat.generate(image_path, emotion='awe')
print(caption)  # "This painting evokes awe through its grand scale and harmonious colors"
```

---

## 15. M√âTRICAS E RESULTADOS

### 15.1 Performance por Emo√ß√£o (v1 Neural)

| Emo√ß√£o | Accuracy | Count (test) |
|--------|----------|--------------|
| contentment | 79.73% | 14,662 |
| sadness | 84.78% | 13,757 |
| fear | 80.71% | 10,282 |
| awe | 55.24% | 8,001 |
| something_else | 57.83% | 5,198 |
| disgust | 57.00% | 5,114 |
| amusement | 58.35% | 4,931 |
| excitement | 47.40% | 4,386 |
| anger | 49.26% | 2,026 |

### 15.2 Observa√ß√µes
- **Melhores**: sadness, fear, contentment (emo√ß√µes "√≥bvias" visualmente)
- **Piores**: excitement, anger (dependem mais do contexto/texto)
- **Fuzzy pode ajudar**: nas emo√ß√µes com forte correla√ß√£o visual

---

## 16. CONCLUS√ÉO

O **Cerebrum Artis** √© um sistema modular e extens√≠vel que combina:

1. **Deep Learning** para alta accuracy (70.23%)
2. **L√≥gica Fuzzy** para explicabilidade (18 regras interpret√°veis)
3. **XAI Visual** para transpar√™ncia (Grad-CAM)
4. **Gera√ß√£o de Legendas** para articula√ß√£o verbal (SAT)

A arquitetura multiagente permite:
- Testar diferentes estrat√©gias de fus√£o
- Substituir componentes individualmente
- Gerar explica√ß√µes em m√∫ltiplos n√≠veis (visual, textual, simb√≥lico)

**Status**: Sistema core funcional, com experimentos de fus√£o em andamento.

---

## 17. PR√ìXIMOS PASSOS (ROADMAP)

### 17.1 Curto Prazo (Agente 2 - SAT Integration) ‚è≥

**Objetivo**: Implementar gera√ß√£o autom√°tica de captions no Agente 2

**Tarefas**:
1. ‚úÖ Agente 2 classe completa (`percepto.py`)
2. ‚è≥ Integrar SAT no m√©todo `generate_caption()`
3. ‚è≥ Testar gera√ß√£o autom√°tica de captions
4. ‚è≥ Validar captions geradas fazem sentido por emo√ß√£o

**Arquivo**: `cerebrum_artis/agents/percepto.py`

**C√≥digo a implementar**:
```python
def generate_caption(self, image_path: str, emotion: Optional[str] = None) -> str:
    """Gera caption afetiva usando SAT"""
    if self.sat_model is None:
        self._load_sat_model()  # Lazy loading
    
    # Carregar imagem
    image = self._preprocess_image(image_path)
    
    # Gerar caption
    with torch.no_grad():
        caption = self.sat_model.generate(
            image.unsqueeze(0).to(self.device),
            emotion=emotion
        )
    
    return caption
```

**Tempo estimado**: 1-2 horas

---

### 17.2 M√©dio Prazo (Agente 3 - Grad-CAM) ‚è≥

**Objetivo**: Implementar explicabilidade visual com Grad-CAM

**Tarefas**:
1. ‚è≥ Criar `cerebrum_artis/agents/explicador.py`
2. ‚è≥ Instalar `pytorch-grad-cam`
3. ‚è≥ Implementar `ExplicadorVisual` class
4. ‚è≥ Gerar heatmaps sobrepostos nas imagens
5. ‚è≥ Testar em imagens de teste

**Arquivo**: `cerebrum_artis/agents/explicador.py`

**Depend√™ncias**:
```bash
pip install grad-cam==1.4.8
```

**C√≥digo base**:
```python
from pytorch_grad_cam import GradCAM
from pytorch_grad_cam.utils.image import show_cam_on_image

class ExplicadorVisual:
    def __init__(self, model, target_layer):
        self.cam = GradCAM(model=model, target_layers=[target_layer])
    
    def explain(self, image_path, target_emotion_idx):
        """Gera heatmap Grad-CAM para emo√ß√£o espec√≠fica"""
        # ... implementa√ß√£o
        return heatmap_overlay
```

**Tempo estimado**: 3-4 horas

---

### 17.3 Longo Prazo (Fus√£o Adaptativa) ‚è≥

**Objetivo**: Combinar Agente 1 + Agente 2 com fus√£o inteligente

**Estrat√©gias a implementar**:

#### A) Fus√£o por Concord√¢ncia (v4 style)
```python
if fuzzy_confidence > 0.8 and neural_confidence > 0.8:
    # Ambos concordam ‚Üí confia mais na predi√ß√£o
    weight_neural = 0.95
else:
    # Discord√¢ncia ‚Üí m√©dia ponderada
    weight_neural = 0.7
```

#### B) Fus√£o por Tipo de Emo√ß√£o
```python
# Emo√ß√µes "visuais" ‚Üí mais peso fuzzy
if emotion in ['sadness', 'contentment']:
    weight_fuzzy = 0.4
# Emo√ß√µes "contextuais" ‚Üí mais peso neural
elif emotion in ['excitement', 'amusement']:
    weight_fuzzy = 0.1
```

#### C) Ensemble Learning
- Treinar meta-modelo (XGBoost/LightGBM) que aprende quando confiar em cada agente
- Features: `[fuzzy_probs, neural_probs, image_features, concordancia]`

**Arquivo**: `cerebrum_artis/fusion/adaptive_fusion.py`

**Tempo estimado**: 5-7 horas

---

### 17.4 Aguardando Treinamento ‚è≥

**v3 e v4** ainda treinando (epoch 2/20):
- **v3**: 69.69% val accuracy (epoch 2)
- **v4**: 64-65% val accuracy (epoch 2)

**A√ß√µes ap√≥s conclus√£o**:
1. ‚úÖ Comparar v3 vs v1 (fuzzy features ajudaram?)
2. ‚úÖ Comparar v4 vs v1 (gating inteligente funciona?)
3. ‚úÖ Atualizar RELATORIO com novos resultados
4. ‚úÖ Escolher melhor vers√£o para produ√ß√£o

**Tempo estimado**: Aguardar ~18 horas (treino GPU)

---

### 17.5 Prioriza√ß√£o Recomendada

**OP√á√ÉO A - Completar Agente 2 primeiro (SAT)**
```
1. Implementar generate_caption() (1h)
2. Testar SAT integration (30min)
3. Atualizar test_percepto.py (30min)
4. Validar captions geradas (1h)
TOTAL: ~3 horas
```

**OP√á√ÉO B - Pular para Agente 3 (Grad-CAM)**
```
1. Criar explicador.py (2h)
2. Instalar pytorch-grad-cam (10min)
3. Implementar GradCAM wrapper (1h)
4. Testar em imagens (1h)
TOTAL: ~4 horas
```

**OP√á√ÉO C - Aguardar v3/v4 e analisar resultados**
```
1. Monitorar treino (~18h)
2. An√°lise comparativa (2h)
3. Atualizar documenta√ß√£o (1h)
TOTAL: ~21 horas (maioria passiva)
```

---

**Decis√£o**: Qual caminho seguir?

1. **SAT Integration** ‚Üí Completa Agente 2 (auto-caption)
2. **Grad-CAM** ‚Üí Implementa Agente 3 (XAI visual)
3. **Aguardar v3/v4** ‚Üí An√°lise comparativa primeiro

---

*Relat√≥rio atualizado em: 2024-11-21 22:30*
*Projeto: Cerebrum Artis - An√°lise Afetiva de Arte*

